# Arquitectura T√©cnica üèóÔ∏è

Este documento describe la arquitectura interna del sistema para facilitar el entendimiento de su funcionamiento por parte de desarrolladores e IAs.

## üèóÔ∏è Resumen de Componentes

El sistema sigue una arquitectura de microservicios orquestada con **Docker Compose**:

1.  **Frontend (React + Vite):** Dashboard SPA para visualizaci√≥n, filtrado y disparo manual de tareas.
2.  **API (FastAPI):** Punto de entrada para el frontend, gesti√≥n de base de datos y encolamiento de tareas.
3.  **Worker (Celery):** Motor de procesamiento en segundo plano que ejecuta los scrapers usando **Playwright**.
4.  **Database (PostgreSQL 16):** Persistencia de inmuebles, estados y b√∫squedas guardadas.
5.  **Broker (Redis):** Cola de mensajes para Celery y cach√© temporal.
6.  **Proxy (Nginx):** Reverse proxy para producci√≥n, manejando el tr√°fico hacia el frontend y backend.

---

## üïµÔ∏è Sistema de Scraping Inteligente

### 1. BaseScraper
Todos los scrapers heredan de una clase base (`backend/scrapers/base.py`) que estandariza:
- Inicializaci√≥n de **Playwright** (User-Agents, Headless mode).
- Navegaci√≥n con manejo de timeouts.
- **Procesamiento de Inmuebles:** L√≥gica base para decidir si un inmueble es nuevo, una actualizaci√≥n de precio o ya existe.
- **Detecci√≥n de Parada:** Si encuentra $N$ registros consecutivos ya existentes, detiene el proceso para ahorrar recursos.

### 2. Estrategia de Mapeo Curado de Alta Precisi√≥n
A diferencia de los sistemas tradicionales, este monitor utiliza un enfoque de "Recolecci√≥n Amplia y Clasificaci√≥n Manual":
- **Nivel de Scraper:** Recolecta todo lo disponible en el Valle de Aburr√° (Medell√≠n, Envigado, Itag√º√≠, Sabaneta, La Estrella) para no perder datos por variaciones de texto.
- **Mapeo de Barrios (`neighborhood_map.json`):** Archivo maestro curado manualmente con +200 variantes mapeadas a barrios est√°ndar. Utiliza un orden de precedencia estricto para evitar colisiones entre barrios con nombres similares en diferentes comunas.
- **Normalizaci√≥n en Base de Datos:** Los inmuebles se procesan mediante `neighborhood_utils.py` para asignar un valor al campo `neighborhood_resolved`, que es el √∫nico utilizado para el filtrado en el Dashboard, garantizando precisi√≥n absoluta.

---

## üóÉÔ∏è Modelo de Datos

### Propiedades (`Property`)
- **Campos principales:** T√≠tulo, precio, ubicaci√≥n, link, imagen, fuente, √°rea, habitaciones, ba√±os.
- **Estados:** `NEW` (reci√©n descubierto), `SEEN` (visto), `ARCHIVED` (descartado), `FAVORITE` (destacado).
- **Indicador de Frescura:** Calculado en tiempo real comparando la fecha de creaci√≥n con la fecha actual.

---

## üöÄ Flujo de Despliegue (VPS)

El despliegue est√° automatizado mediante un script de PowerShell y un flujo de Git:
1.  **Local:** Ajustes de c√≥digo y commits.
2.  **GitHub:** Push a `main`.
3.  **VPS (CI/CD Manual):** `git pull` + `docker compose restart`.
4.  **Infraestructura:** VPS Ubuntu 24.04 con Docker.

---

## üõ†Ô∏è Extensibilidad

Para a√±adir un nuevo portal inmobiliario:
1.  Investigar selectores en `nueva-url.md`.
2.  Crear clase en `backend/scrapers/`.
3.  Registrar en `backend/scrapers/factory.py`.
4.  A√±adir a la lista de `PORTALS` en `frontend/src/App.jsx`.
5.  Actualizar la lista blanca en el endpoint `/scrape` de `backend/main.py`.

---

## ü§ñ Workflows Automatizados (Agentic Flows)

El proyecto incluye flujos de trabajo en `.agent/workflows/` para automatizar tareas repetitivas:
- **`/validar-url`:** Un flujo E2E que valida un portal, crea el c√≥digo del scraper, lo integra en el frontend y lo despliega en el VPS autom√°ticamente.
- **`/setup-local-dev`:** Configura t√∫neles SSH hacia el VPS para usar la base de datos de producci√≥n desde el entorno local.
- **`/git-update`:** Automatiza el ciclo de add, commit, push y actualizaci√≥n del VPS.
